<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Online Fraud Detection with Imbalanced Learning: A Practical Pipeline and Evaluation</title>
<style>
  :root {
    --fg: #111;
    --sub: #444;
    --muted: #666;
    --bg: #fff;
    --accent: #0b6efd;
  }
  html, body { background: var(--bg); color: var(--fg); font: 16px/1.6 system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif; }
  main { max-width: 880px; margin: 2rem auto; padding: 0 1rem 5rem; }
  header h1 { font-size: 1.8rem; margin: 0 0 0.25rem; }
  header .meta { color: var(--muted); font-size: 0.95rem; }
  h2 { margin-top: 2rem; border-top: 2px solid #eee; padding-top: 1rem; }
  h3 { margin-top: 1.2rem; color: var(--sub); }
  figure { margin: 1rem 0; }
  figcaption { color: var(--muted); font-size: 0.9rem; }
  code, pre { background: #f7f7f9; border: 1px solid #eee; padding: 0.15rem 0.35rem; border-radius: 4px; }
  table { width: 100%; border-collapse: collapse; margin: 0.5rem 0 1rem; }
  table.metrics th, table.metrics td { border: 1px solid #ddd; padding: 8px 10px; text-align: left; }
  table.metrics th { background: #fafafa; }
  .callout { background: #f2f8ff; border: 1px solid #dbeafe; padding: 0.75rem 1rem; border-radius: 8px; }
  .small { color: var(--muted); font-size: 0.95rem; }
  @media print {
    header, footer { break-after: avoid; }
    h2 { break-before: page; }
  }
</style>
</head>
<body>
<main>
  <header>
    <h1>Online Fraud Detection with Imbalanced Learning: A Practical Pipeline and Evaluation</h1>
    <div class="meta">Author: [Your Name] &nbsp;•&nbsp; November 01, 2025</div>
  </header>

  <section id="abstract">
    <h2>Abstract</h2>
    <p>
      This paper documents a reproducible pipeline for detecting online transaction fraud using
      modern imbalanced-learning techniques. Starting from a raw tabular CSV, we apply schema-agnostic
      preprocessing (imputation, scaling, and one-hot encoding), class-imbalance handling (SMOTE and
      class weighting), and train three competitive baselines&mdash;Logistic Regression, Random Forest,
      and Gradient Boosting (XGBoost). We evaluate models with ROC AUC and Average Precision (PR AUC),
      and we select the operating point by tuning the decision threshold to maximize F<sub>2</sub>,
      reflecting a recall-heavy business objective. On the present sample, Random Forest achieves the
      best PR AUC and ROC AUC and, at a tuned threshold, attains a zero-error confusion matrix on the
      small hold-out set. We discuss overfitting risks and outline safeguards for production deployment.
    </p>
    <p class="small"><strong>Keywords:</strong> fraud detection, imbalanced learning, SMOTE, ROC AUC,
      average precision, threshold tuning, Random Forest, XGBoost.</p>
  </section>

  <section id="intro">
    <h2>1. Introduction</h2>
    <p>
      Online payments remain a prime target for adversarial behavior. Accurate, fast, and interpretable
      fraud detection is essential for minimizing financial loss while preserving customer experience.
      Class imbalance is the central challenge: fraudulent events are rare relative to legitimate ones,
      which biases naive learners toward the majority class. This work presents a practical training and
      evaluation recipe that can be reused across datasets with minimal schema assumptions.
    </p>
  </section>

  <section id="data">
    <h2>2. Data</h2>
    <p>
      The notebook auto-detected the label column as <code>isFraud</code>, with the following class ratio
      extracted from the saved execution outputs:
    </p>
    <pre class="small">Label column: isFraud
isFraud
0    0.683333
1    0.316667
Name: ratio, dtype: float64</pre>
    <p>
      After an 80/20 stratified split, the shapes were:
    </p>
    <pre class="small">Train: (48, 11) Test: (12, 11)</pre>
    <p>
      The training set positive rate and imbalance options logged:
    </p>
    <pre class="small">Positives in train: 15 of 48 (pos_rate=0.3125)
SMOTE: True | scale_pos_weight for XGB: 2.2</pre>
    <figure><img src='class_balance.png' alt='Class balance' style='width:100%;max-width:520px;'><figcaption>Figure 1. Class balance bar chart saved by the notebook.</figcaption></figure>
    <p class="small">
      Note: The hold-out test set contains only 12 examples (4 positives), which inflates metrics and mandates
      cross-validation and external validation before deployment.
    </p>
  </section>

  <section id="methods">
    <h2>3. Methods</h2>
    <h3>3.1 Preprocessing</h3>
    <p>
      Numerical features were median-imputed and standardized; categorical features were imputed with the
      most frequent value and one-hot encoded. Processing was implemented via a scikit-learn
      <code>ColumnTransformer</code> within a pipeline to avoid leakage.
    </p>
    <h3>3.2 Imbalance handling</h3>
    <p>
      We used SMOTE oversampling on the training folds (inside the pipeline) and also leveraged
      class-weighting for Logistic Regression and Random Forest. For XGBoost, <code>scale_pos_weight</code>
      would be used when SMOTE is disabled.
    </p>
    <h3>3.3 Models</h3>
    <ul>
      <li><strong>Logistic Regression</strong> (balanced class weights, max_iter=2000)</li>
      <li><strong>Random Forest</strong> (n_estimators=400, balanced_subsample)</li>
      <li><strong>XGBoost</strong> (500 trees, max_depth=6, learning_rate=0.06, histogram tree method)</li>
    </ul>
    <h3>3.4 Evaluation</h3>
    <p>
      We report ROC AUC and Average Precision (PR AUC). To pick an operating point, we tuned the threshold
      on the validation split to maximize F<sub>2</sub>, prioritizing recall. The code also prints
      classification reports and confusion matrices at both the default 0.50 threshold and the tuned threshold.
    </p>
  </section>

  <section id="results">
    <h2>4. Results</h2>
    <div class="callout">
      <pre class="small">Best by AP: RF AP= 1.0 ROC AUC= 1.0
Best F2 threshold ~ 0.435 | Precision=1.000 Recall=1.000 F2=1.000

Classification report @ tuned threshold:
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     1.0000    1.0000    1.0000         4

    accuracy                         1.0000        12
   macro avg     1.0000    1.0000    1.0000        12
weighted avg     1.0000    1.0000    1.0000        12

RF Confusion Matrix @ thr=0.435
        Pred 0  Pred 1
True 0       8       0
True 1       0       4
TN=8 FP=0 FN=0 TP=4</pre>
    </div>
    <p>Summary metrics (hold-out set):</p>
    <table class='metrics'>
<thead><tr><th>Model</th><th>ROC AUC</th><th>Average Precision (PR AUC)</th></tr></thead>
<tbody>
<tr><td>LogReg</td><td>0.9375</td><td>0.9167</td></tr>
<tr><td>RF</td><td>1.0000</td><td>1.0000</td></tr>
<tr><td>XGB</td><td>0.9688</td><td>0.9500</td></tr>
</tbody>
</table>
    <p class="small">
      The notebook identified Random Forest as best by Average Precision (and ROC AUC). With a tuned threshold
      (≈0.43 in the saved output), the confusion matrix shows TN=8, FP=0, FN=0, TP=4 on 12 test samples, i.e., 100% on this
      small split. Treat these values as optimistic; repeat with K-fold cross-validation and a larger test set.
    </p>
  </section>

  <section id="discussion">
    <h2>5. Discussion</h2>
    <h3>5.1 Error Costs and Thresholds</h3>
    <p>
      In fraud detection, missed fraud (FN) usually costs more than false alarms (FP). Thresholds should be
      tuned to minimize expected cost, not to maximize generic accuracy. The F<sub>2</sub>-oriented tuning is
      a reasonable proxy when explicit cost curves are unavailable.
    </p>
    <h3>5.2 Robustness and Overfitting</h3>
    <ul>
      <li>Perform stratified K-fold cross-validation (e.g., 5–10 folds) and report variance.</li>
      <li>Hold out a calendar-based test set to simulate temporal drift.</li>
      <li>Stress-test with adversarial examples and rare-pattern injections.</li>
    </ul>
    <h3>5.3 Fairness and Abuse Prevention</h3>
    <p>
      Audit feature importances and error rates across subgroups to prevent disparate impact. Use
      privacy-preserving techniques when joining external signals. Avoid learning on directly sensitive
      attributes; prefer derived, behavior-based signals.
    </p>
  </section>

  <section id="deployment">
    <h2>6. Deployment Considerations</h2>
    <ul>
      <li>Serve the pipeline as a versioned artifact; store the tuned threshold with the model.</li>
      <li>Add real-time monitoring for population drift, alert rate, and approval reversal rate (chargebacks).</li>
      <li>Recalibrate probabilities periodically (Platt/Isotonic) as base rates shift.</li>
      <li>Shadow-test new models with canaries or A/B rollouts before full adoption.</li>
    </ul>
  </section>

  <section id="conclusion">
    <h2>7. Conclusion</h2>
    <p>
      A simple, rigorously evaluated pipeline already achieves strong fraud discrimination on the sample.
      The next steps are to validate with cross-validation, expand features, encode cost asymmetries directly,
      and productionize with strict monitoring and retraining triggers.
    </p>
  </section>

  <section id="refs">
    <h2>References</h2>
    <ol>
      <li>Chawla, N. V., et al. “SMOTE: Synthetic Minority Over-sampling Technique.” <em>Journal of Artificial Intelligence Research</em>, 2002.</li>
      <li>Davis, J., and Goadrich, M. “The Relationship Between Precision-Recall and ROC Curves.” <em>ICML</em>, 2006.</li>
      <li>Chen, T., and Guestrin, C. “XGBoost: A Scalable Tree Boosting System.” <em>KDD</em>, 2016.</li>
      <li>Pedregosa, F., et al. “Scikit-learn: Machine Learning in Python.” <em>JMLR</em>, 2011.</li>
      <li>Lemaître, G., Nogueira, F., &amp; Aridas, C. K. “Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets.” <em>JMLR</em>, 2017.</li>
    </ol>
  </section>

  <footer class="small">
    Generated from the saved outputs of <code>online_fraud_detection.ipynb</code>.
  </footer>
</main>
</body>
</html>
